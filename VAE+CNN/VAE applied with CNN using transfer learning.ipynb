{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Encoder Theory Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variational ecoder(VAE) has been widely adopted. In the application level, it can be slow to train because of the noise it introduces to the encoded layer. In this notebook, we try to train a VAE with a convolution network. \n",
    "\n",
    "The strategy is to first train an convolutional auto-encoder first and then free the encoder and implement the VAE structure to map the feature layer to a variational latent space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application Highlights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./train-images-idx3-ubyte.gz\n",
      "Extracting ./train-labels-idx1-ubyte.gz\n",
      "Extracting ./t10k-images-idx3-ubyte.gz\n",
      "Extracting ./t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.examples.tutorials.mnist import input_data    \n",
    "FLAGS = None\n",
    "mnist = input_data.read_data_sets('./', one_hot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard CMD:tensorboard --logdir=C:/Users/CK/Documents/tf_logs20181011-181705/\n"
     ]
    }
   ],
   "source": [
    "#Clearing all\n",
    "tf.reset_default_graph()\n",
    "#Opening new directory for tensorboard\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "logdir = 'C:/Users/CK/Documents/tf_logs' + now.strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "#To monitor training through tensorboard\n",
    "print('TensorBoard CMD:tensorboard --logdir='+logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "batch_size = 800\n",
    "inputs_ = tf.placeholder(tf.float32, (batch_size, 28, 28, 1), name='inputs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def featConstructor(inputs,varScope,batch_normalised=True,activation=tf.nn.sigmoid):\n",
    "    \n",
    "    with tf.variable_scope(varScope):\n",
    "        norm1=tf.layers.batch_normalization(inputs_,training=batch_normalised,axis=-1)\n",
    "        conv1 = tf.layers.separable_conv2d(\n",
    "                norm1,\n",
    "                filters=32,\n",
    "                kernel_size=(3,3),\n",
    "                strides=(1, 1),\n",
    "                activation=activation\n",
    "        )\n",
    "        maxpool1 = tf.layers.max_pooling2d(conv1, pool_size=(2,2), strides=(2,2), padding='same')\n",
    "        norm2=tf.layers.batch_normalization(maxpool1,training=batch_normalised,axis=-1)\n",
    "        conv2 = tf.layers.separable_conv2d(\n",
    "                inputs=norm2,\n",
    "                filters=16,\n",
    "                kernel_size=(3,3),\n",
    "                strides=(1, 1),\n",
    "                activation=activation\n",
    "        )\n",
    "        maxpool2 = tf.layers.max_pooling2d(conv2, pool_size=(2,2), strides=(2,2), padding='same')\n",
    "        norm3=tf.layers.batch_normalization(maxpool2,training=batch_normalised,axis=-1)\n",
    "        conv3= tf.layers.separable_conv2d(\n",
    "                inputs=norm3,\n",
    "                filters=8,\n",
    "                kernel_size=(3,3),\n",
    "                strides=(1, 1),\n",
    "                activation=activation\n",
    "        )\n",
    "        return tf.layers.max_pooling2d(conv3, pool_size=(2,2), strides=(2,2), padding='same',name='encoded') # Now 2x2x8\n",
    "\n",
    "def imgConstructor(features,varScope,batch_normalised=True, activation=tf.nn.relu):\n",
    "    with tf.variable_scope(varScope):\n",
    "        norm1=tf.layers.batch_normalization(\n",
    "            features,\n",
    "            training=batch_normalised,\n",
    "            axis=-1\n",
    "        )\n",
    "        upsample1 = tf.image.resize_images(norm1, size=(7,7), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "        conv4 = tf.layers.separable_conv2d(inputs=upsample1,\n",
    "                filters=16,\n",
    "                kernel_size=(3,3),\n",
    "                strides=(1, 1),\n",
    "                padding='same',\n",
    "                data_format='channels_last',\n",
    "                dilation_rate=(1, 1),\n",
    "                depth_multiplier=2,\n",
    "                activation=activation)\n",
    "        norm2=tf.layers.batch_normalization(\n",
    "            conv4,\n",
    "            training=batch_normalised,\n",
    "            axis=-1\n",
    "        )\n",
    "        upsample2 = tf.image.resize_images(norm2, size=(14,14), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "        conv5 = tf.layers.separable_conv2d(inputs=upsample2,\n",
    "                filters=32,\n",
    "                kernel_size=(3,3),\n",
    "                strides=(1, 1),\n",
    "                padding='same',\n",
    "                data_format='channels_last',\n",
    "                dilation_rate=(1, 1),\n",
    "                depth_multiplier=2,\n",
    "                activation=activation)\n",
    "        norm3=tf.layers.batch_normalization(\n",
    "            conv5,\n",
    "            training=batch_normalised,\n",
    "            axis=-1\n",
    "        )\n",
    "        upsample3 = tf.image.resize_images(norm3, size=(28,28), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "        conv6 = tf.layers.separable_conv2d(inputs=upsample3, \n",
    "                filters=64,\n",
    "                kernel_size=(3,3),\n",
    "                strides=(1, 1),\n",
    "                padding='same',\n",
    "                data_format='channels_last',\n",
    "                dilation_rate=(1, 1),\n",
    "                depth_multiplier=2,\n",
    "                activation=activation)\n",
    "        norm4=tf.layers.batch_normalization(\n",
    "            conv6,\n",
    "            training=batch_normalised,\n",
    "            axis=-1\n",
    "        )\n",
    "        logits = tf.layers.separable_conv2d(inputs=norm4, filters=1, kernel_size=(3,3), padding='same', activation=None)\n",
    "        # Pass logits through sigmoid to get reconstructed image\n",
    "        return logits,tf.nn.sigmoid(logits)\n",
    "def encoder(inputs,featConstructor,imgConstructor,varScope):\n",
    "    with tf.variable_scope(varScope):\n",
    "        features=featConstructor(inputs,'features')\n",
    "        images_logits,images=imgConstructor(features,'images')\n",
    "        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=inputs, logits=images_logits))\n",
    "        with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "            adam = tf.train.AdamOptimizer(learning_rate)\n",
    "            # Return the encoded code for embedding visualisation, the optimiser for executing training session.\n",
    "            return features,adam.minimize(loss,var_list= tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,scope=varScope))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feats,opt_feats=encoder(inputs_,featConstructor,imgConstructor,'CNN_encoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A multi-layer coder \n",
    "def coder(inputs,layer_units,varScope,batch_normalised=True,activation=tf.nn.relu):\n",
    "    with tf.variable_scope(varScope):\n",
    "        layers=[inputs]\n",
    "        for i in range(0,len(layer_units)):\n",
    "            print(layers[-1])\n",
    "            layers=layers+[tf.layers.batch_normalization(layers[-1],training=batch_normalised,axis=-1)]\n",
    "            layers=layers+[tf.layers.dense(layers[-1],units=layer_units[i],activation=tf.nn.relu)]\n",
    "    return tf.layers.dense(layers[-1],units=layer_units[-1],activation=activation)\n",
    "# A sampler to sample variations from the latent space\n",
    "def sampler(mean,sd):\n",
    "    epsilon_=tf.random_normal(shape=[i.value for i in mean.get_shape()])\n",
    "    #return mean + tf.exp(0.5 * logVar) * epsilon_\n",
    "    sample=mean + sd*epsilon_\n",
    "    return sample\n",
    "# A variational ecoder using coder input\n",
    "def vae(inputs,coder,encoder_units,decoder_units,varScope):\n",
    "    original_shape=inputs.get_shape()\n",
    "    inputs_flattened=tf.reshape(inputs,shape=(original_shape[0].value,-1))\n",
    "    with tf.variable_scope(varScope):\n",
    "        encoded_mean=coder(inputs_flattened,encoder_units,'encoded_mean')\n",
    "        encoded_sd=coder(inputs_flattened,encoder_units,'encoded_sd')\n",
    "        encoded=sampler(encoded_mean,encoded_sd)\n",
    "        logits=coder(encoded,decoder_units,'decoded',activation=None)\n",
    "        decoded=tf.nn.sigmoid(logits)\n",
    "        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=inputs_flattened, logits=logits))\n",
    "        kl_loss = -0.5*tf.reduce_mean(1 + encoded_sd - tf.square(encoded_mean) - tf.exp(encoded_sd))\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "        adam = tf.train.AdamOptimizer(learning_rate)\n",
    "    \n",
    "    # Return the encoded code for embedding visualisation, the optimiser for executing training session.\n",
    "    return encoded,adam.minimize(loss+0.1*kl_loss,var_list= tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,scope=varScope))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../CNN/CNN_encoder.ckpt\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, \"../CNN/CNN_encoder.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape_1:0\", shape=(800, 32), dtype=float32)\n",
      "Tensor(\"vae/encoded_mean/dense/Relu:0\", shape=(800, 16), dtype=float32)\n",
      "Tensor(\"Reshape_1:0\", shape=(800, 32), dtype=float32)\n",
      "Tensor(\"vae/encoded_sd/dense/Relu:0\", shape=(800, 16), dtype=float32)\n",
      "Tensor(\"vae/add:0\", shape=(800, 8), dtype=float32)\n",
      "Tensor(\"vae/decoded/dense/Relu:0\", shape=(800, 16), dtype=float32)\n",
      "[None, <tf.Operation 'Adam' type=NoOp>, <tf.Tensor 'vae/add:0' shape=(800, 8) dtype=float32>]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Fetch argument None has invalid type <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-95-a3cd092e66e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mimgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_xs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0msummary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmerged_summary\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoded\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0minputs_\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mimgs\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Running epochs {}, batch {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mii\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[1;32m-> 1120\u001b[1;33m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[0;32m   1121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m     \u001b[1;31m# Run request and get response.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[0;32m    425\u001b[0m     \"\"\"\n\u001b[0;32m    426\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 427\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    428\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_targets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[1;34m(fetch)\u001b[0m\n\u001b[0;32m    243\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m       \u001b[1;31m# NOTE(touts): This is also the code path for namedtuples.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 245\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0m_ListFetchMapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_DictFetchMapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fetches)\u001b[0m\n\u001b[0;32m    350\u001b[0m     \"\"\"\n\u001b[0;32m    351\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mappers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_FetchMapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfetch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unique_fetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_uniquify_fetches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mappers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    350\u001b[0m     \"\"\"\n\u001b[0;32m    351\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mappers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_FetchMapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfetch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unique_fetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_uniquify_fetches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mappers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[1;34m(fetch)\u001b[0m\n\u001b[0;32m    240\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfetch\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m       raise TypeError('Fetch argument %r has invalid type %r' % (fetch,\n\u001b[1;32m--> 242\u001b[1;33m                                                                  type(fetch)))\n\u001b[0m\u001b[0;32m    243\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m       \u001b[1;31m# NOTE(touts): This is also the code path for namedtuples.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Fetch argument None has invalid type <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "#Training model\n",
    "\n",
    "vae_inputs=tf.reshape(feats,shape=(batch_size,-1))\n",
    "encoded,opt=vae(vae_inputs,coder,[16,8],[16,32],'vae')\n",
    "\n",
    "merged_summary=tf.summary.merge_all()\n",
    "print([merged_summary,opt,encoded])\n",
    "modelname=\"CNN_feats_VAE\"\n",
    "writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "for e in range(epochs):\n",
    "    for ii in range(mnist.train.num_examples//batch_size):\n",
    "        \n",
    "        batch_xs,batch_ys = mnist.train.next_batch(batch_size)\n",
    "        imgs = batch_xs.reshape((-1, 28, 28, 1)) \n",
    "        \n",
    "        summary, _, code = sess.run([merged_summary,opt,encoded], feed_dict={inputs_: imgs})\n",
    "        \n",
    "        print(\"Running epochs {}, batch {}\".format(e+1,ii+1))\n",
    "        writer.add_summary(summary, ii+batch_size*e)\n",
    "\n",
    "    # Add ops to save and restore all the variables.\n",
    "    saver.save(sess, \"./\"+modelname+\".ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#To show the tensorBoard\n",
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = \"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))\n",
    "show_graph(tf.get_default_graph().as_graph_def())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Embeddings(code) visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_sprite_image(images):\n",
    "    \"\"\"Returns a sprite image consisting of images passed as argument. Images should be count x width x height\"\"\"\n",
    "    if isinstance(images, list):\n",
    "        images = np.array(images)\n",
    "    img_h = images.shape[1]\n",
    "    img_w = images.shape[2]\n",
    "    n_plots = int(np.ceil(np.sqrt(images.shape[0])))\n",
    "    \n",
    "    \n",
    "    spriteimage = np.ones((img_h * n_plots ,img_w * n_plots ))\n",
    "    \n",
    "    for i in range(n_plots):\n",
    "        for j in range(n_plots):\n",
    "            this_filter = i * n_plots + j\n",
    "            if this_filter < images.shape[0]:\n",
    "                this_img = images[this_filter]\n",
    "                spriteimage[i * img_h:(i + 1) * img_h,\n",
    "                  j * img_w:(j + 1) * img_w] = this_img\n",
    "    \n",
    "    return spriteimage\n",
    "\n",
    "def vector_to_matrix_mnist(mnist_digits):\n",
    "    \"\"\"Reshapes normal mnist digit (batch,28*28) to matrix (batch,28,28)\"\"\"\n",
    "    return np.reshape(mnist_digits,(-1,28,28))\n",
    "\n",
    "def invert_grayscale(mnist_digits):\n",
    "    \"\"\" Makes black white, and white black \"\"\"\n",
    "    return 1-mnist_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "to_visualise = batch_xs\n",
    "to_visualise = vector_to_matrix_mnist(to_visualise)\n",
    "to_visualise = invert_grayscale(to_visualise)\n",
    "\n",
    "sprite_image = create_sprite_image(to_visualise)\n",
    "plt.imsave(os.path.join(logdir,'mnistdigits.png'),sprite_image,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(logdir,'metadata.tsv'),'w') as f:\n",
    "    f.write(\"Index\\tLabel\\n\")\n",
    "    for index,label in enumerate(batch_ys):\n",
    "        f.write(\"%d\\t%d\\n\" % (index,label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/CK/Documents/tf_logs20181011-131207/embeddings.ckpt'"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embeddings logging for Tensorboard\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "config = projector.ProjectorConfig()\n",
    "embedding_var=tf.Variable(code,dtype=tf.float32,name='embeddings')\n",
    "embedding = config.embeddings.add()\n",
    "embedding.tensor_name = embedding_var.name\n",
    "sess.run(embedding_var.initializer)\n",
    "summary_writer = tf.summary.FileWriter(logdir)\n",
    "# Specify where you find the metadata\n",
    "embedding.metadata_path = os.path.join(logdir,'metadata.tsv') #'metadata.tsv'\n",
    "\n",
    "# Specify where you find the sprite (we will create this later)\n",
    "embedding.sprite.image_path = os.path.join(logdir,'mnistdigits.png') #'mnistdigits.png'\n",
    "embedding.sprite.single_image_dim.extend([28,28])\n",
    "\n",
    "# Say that you want to visualise the embeddings\n",
    "projector.visualize_embeddings(summary_writer, config)\n",
    "saver = tf.train.Saver([embedding_var])\n",
    "saver.save(sess, os.path.join(logdir, 'embeddings.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
